{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REPORT Project 1: Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Learning (DQN)\n",
    "Deep Q-Learning is Q-learning in conjunction with neural networks. This combination has the name DQN. The implement of the algorithm is mostly based on the paper, [Human-level control through deep reinforcement\n",
    "learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf).\n",
    "\n",
    "#### Implementation\n",
    "- Initialize replay memory $D$ with capacity $N$\n",
    "- Initialize action-value function $\\hat{q}$ with random $\\text{w}$\n",
    "- Initialize target action-value weights $\\text{w}^{-}\\leftarrow \\text{w}$\n",
    "- Initialize Greedy epsilon $\\epsilon$\n",
    "- for the episode $e \\leftarrow 1$ to $M$:\n",
    "    - Initialize state $S$\n",
    "    - for time step $t \\leftarrow 1$ to $T$:\n",
    "        - Choose action $A$ from state $S$ using policy $\\pi\\leftarrow\\epsilon-\\text{Greedy}(\\hat{q}(S,A,\\text{w}))$\n",
    "        - Take action $A$, observe reward $R$, and next state $S^{'}$\n",
    "        - Store experience tuple $(S,A,R,S^{'})$ in replay memory $D$\n",
    "        - $S \\leftarrow S^{'}$\n",
    "        - Obtain random minibatch of tuples $(s_j,a_j,r_j,s_{j+1})$ from $D$\n",
    "        - Set target $y_j = r_j + \\gamma\\max_a\\hat{q}(s_{j+1},a,\\text{w}^{-})$\n",
    "        - Update: $\\Delta\\text{w} = \\alpha(y_j - \\hat{q}(s_j,a_j,\\text{w}))\\nabla_{\\text{w}}\\hat{q}(s_j,a_j,\\text{w})$\n",
    "        - Every $C$ steps, soft update parameters with $\\tau$: $\\text{w}^{-}\\leftarrow\\tau * \\text{w} + (1 - \\tau) * \\text{w}^{-}$\n",
    "    - Update $\\epsilon\\leftarrow\\epsilon * decay$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "- Buffer size or capacity $(N)$ of replay memory $(D)$: ${10}^5$\n",
    "- Minibatch size                    : $64$\n",
    "- Greedy Policy Epsilon $(\\epsilon)$\n",
    " - Initial: $1.0$\n",
    " - Minimum: $0.01$\n",
    " - Decay: $0.995$\n",
    "- Number of episodes $(M)$: $2,000$\n",
    "- Max step per episode $(T)$: $1,000$\n",
    "- Discount factor or Gamma $(\\gamma)$: $0.99$\n",
    "- Interpolation rate $(\\tau)$ for soft update target model parameters: $10^{-3}$\n",
    "- Update model parameters every $(C)$ steps: 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN model architecture\n",
    "![Basic DQN](images/dqn-model.png)\n",
    "The model or network for learn (local) or target consists of two fully-connected layers with 64 units each.  Each layer is followed by ReLu activation layer.  The input layer size is 37 as of the dimension of the state.  The output layer size is 4 as of the number of actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea came from a paper titled [*Deep Reinfocement Learning with Double Q-Learning (van Hasselt, Guez, and Silver, 2015)*](https://arxiv.org/pdf/1509.06461.pdf). In the paper, the authors demonstrated that the basic DQN has a tendency to overestimate values for Q, which may be harmful to training performance and sometimes can lead to suboptimal policies. The root cause of this is the max operation in the Bellman equation.\n",
    "\n",
    "As a solution to this problem, the authors proposed modifying the Bellman update a bit. The authors of the paper proposed choosing actions for the next state using the trained network but taking values of $\\hat{q}$ from the target net.\n",
    "\n",
    "$$y_j = r_j + \\gamma\\hat{q}(s_{j+1},{\\arg\\max}_a\\hat{q}(s_{j+1},a,\\text{w}),\\text{w}^{-})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dueling DQN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This improvement was proposed in the paper called [Dueling Network Architectures for Deep Reinforcement Learning (Wang et al., 2015)](https://arxiv.org/pdf/1511.06581.pdf).  It brought better training stability, faster convergence and better results on the Atari benchmark. The core concept is that the Q-value $Q(s,a)$ the network is trying to estimate can be devided into the value of the state $V(s)$ and the advantage of actions in this state $A(s)$.\n",
    "\n",
    "$$Q(s, a) = V(s) + A(s, a)$$\n",
    "\n",
    "The intuition behind this is that the value of most states do not vary a lot across actions. It makes sense to try estimating them directly.  But we still need to capture the difference actions make which can be measured by $A(s, a)$.\n",
    "\n",
    "This modification can be done in the network as the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dueling DQN](images/dueling-dqn-udacity.png)\n",
    "$$\\text{A basic DQN (top) and dueling architecture (bottom)}$$\n",
    "$$\\text{Source: Udacity's Deep Reinforcement Learning NanoDegree}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dueling DQN model architecture\n",
    "![Dueling DQN](images/dueling-dqn.png)\n",
    "\n",
    "We add two fully connected layers- one-unit layer for $V(s)$ and four-unit layer for $A(s)$.  The output is the summation of the two layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic DQN**\n",
    "\n",
    "Environment solved in 428 episodes!\tAverage Score: 13.11\n",
    "![Basic DQN](report/basic.png) \n",
    "\n",
    "**Double DQN**\n",
    "\n",
    "Environment solved in 407 episodes!\tAverage Score: 13.00\n",
    "![Double DQN](report/double.png) \n",
    "\n",
    "**Dueling DQN**\n",
    "\n",
    "Environment solved in 390 episodes!\tAverage Score: 13.04\n",
    "![Dueling DQN](report/dueling.png) \n",
    "\n",
    "**Double Dueling DQN**\n",
    "\n",
    "Environment solved in 414 episodes!\tAverage Score: 13.01\n",
    "![Double Dueling DQN](report/double_dueling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons\n",
    "In this project, all DQN with extensions seem to perform slightly better (solved faster) than the basic or vanilla DQN.\n",
    "![Comparision](report/compare.png)\n",
    "\n",
    "## What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There're a lot improvements we can do such as:\n",
    "- Adjust hyperparameters for each algorithms to improve the agent's performance.\n",
    "- Try implement different extensions of DQN such as:\n",
    "  - [Prioritzed experience replay](https://arxiv.org/abs/1511.05952)\n",
    "  - [N-steps DQN](http://incompleteideas.net/papers/sutton-88-with-erratum.pdf)\n",
    "  - [Distributional DQN](https://arxiv.org/abs/1707.06887)\n",
    "  - [Noisy DQN](https://arxiv.org/abs/1706.10295)\n",
    "  - Or the combinations of these extensions [Rainbow](https://arxiv.org/abs/1710.02298)\n",
    "- Try to train an agent from raw pixels! \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
